{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Malware Detection with CNN.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahmed-Mustafa-2005/Computer-Forensics/blob/main/Malware_Detection_with_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgCavCn4TNRB"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import pathlib\n",
        "import copy\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "xQKGF-CXTBi7",
        "outputId": "a7920eb8-0208-482b-fc33-bbe016d60348"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oX5w9Zd1Ifa"
      },
      "source": [
        "!unrar x \"/content/drive/My Drive/malware_dataset/train.tar\" -C \"/content/drive/My Drive/data/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-rUbGZYV1jL"
      },
      "source": [
        "Here we download necessary library to split folders\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZOm9KZ63DC5"
      },
      "source": [
        "!pip install split-folders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mISsz4tTV8yM"
      },
      "source": [
        "This will kill created data folder (only use if you want restart the whole model creation process)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axKxVLCtI5RS"
      },
      "source": [
        "import shutil\n",
        "\n",
        "shutil.rmtree('/content/drive/My Drive/data/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T_kgexB3D5c"
      },
      "source": [
        "import splitfolders\n",
        "splitfolders.ratio('/content/drive/My Drive/data/train', output=\"output\", seed=1337, ratio=(.8, 0.2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-wCATBDSda0"
      },
      "source": [
        "# how many samples per batch to load\n",
        "data_dir= pathlib.Path(os.path.normpath('/content/output/'))\n",
        "train_path = data_dir/\"train\"\n",
        "val_path = data_dir/\"val\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ74KbHI1lhL"
      },
      "source": [
        "malware_train = len(os.listdir(train_path/\"malware\"))\n",
        "bengin_train = len(os.listdir(train_path/\"benign\"))\n",
        "malware_valid = len(os.listdir(val_path/\"malware\"))\n",
        "bengin_valid = len(os.listdir(val_path/\"benign\"))\n",
        "print(\"Malware Train Images: {}\".format(malware_train))\n",
        "print(\"Bengin Train Images: {}\".format(bengin_train))\n",
        "print(\"Malware Validaten Images: {}\".format(malware_valid))\n",
        "print(\"Bengin Validation Images: {}\".format(bengin_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tAH9kk039wT"
      },
      "source": [
        "transform = transforms.Compose([transforms.Resize((256,256)),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu_GKjbJ4CTv"
      },
      "source": [
        "#loading the folders\n",
        "train = datasets.ImageFolder(train_path, transform = transform)\n",
        "val = datasets.ImageFolder(val_path, transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEr-GMMv4DVq"
      },
      "source": [
        "#loading the dataset\n",
        "batch_size = 128\n",
        "train_set = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle = True)\n",
        "val_set = torch.utils.data.DataLoader(val, batch_size = batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8mmfMZv4GTk"
      },
      "source": [
        "classes = os.listdir(train_path)\n",
        "print(classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4qNH-8A4Js3"
      },
      "source": [
        "def imshow(image, title = None):\n",
        "    image = image.numpy().transpose((1,2,0))\n",
        "    mean = np.array([0.485,0.456,0.406])\n",
        "    std = np.array([0.229,0.224,0.225])\n",
        "    image = image*std + mean\n",
        "    image = np.clip(image, 0, 1)\n",
        "    plt.imshow(image)\n",
        "    if title is not None:\n",
        "        print(title)\n",
        "    plt.pause(0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrJBPTcM4M7U"
      },
      "source": [
        "#getting a batch of training data\n",
        "images, labels = next(iter(train_set))\n",
        "out = torchvision.utils.make_grid(images)\n",
        "imshow(out,title=[classes[x] for x in labels])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijtkRcDA4QRw"
      },
      "source": [
        "imshow(images[1],classes[labels[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9QPT7Vz4dVW"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # convolutions\n",
        "        self.conv1 = nn.Conv2d(3,16,3,padding=1)\n",
        "        self.conv2 = nn.Conv2d(16,32,3,padding=1)\n",
        "        self.conv3 = nn.Conv2d(32,64,3,padding=1)\n",
        "        self.conv4 = nn.Conv2d(64,128,3,padding=1)\n",
        "        self.conv5 = nn.Conv2d(128,64,3,padding=1)\n",
        "        # Max-pool\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "\n",
        "        self.fc1 = nn.Linear(256*4*4,1024)\n",
        "        self.fc2 = nn.Linear(1024, 1024)\n",
        "        self.fc3 = nn.Linear(1024, 2)\n",
        "        #self.fc4 = nn.Linear(1024, 512)\n",
        "        #self.fc5 = nn.Linear(512, 2)\n",
        "        # Dropout module with 0.5 drop probability\n",
        "        self.dropout = nn.Dropout(p = 0.5, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # make sure input tensor is flattened\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        print(x.shape)\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        print(x.shape)\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        print(x.shape)\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        print(x.shape)\n",
        "        x = self.pool(F.relu(self.conv5(x)))\n",
        "        print(x.shape)\n",
        "        x = x.view(-1,256*4*4)\n",
        "\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        #x = self.dropout(F.relu(self.fc3(x)))\n",
        "        #x = self.dropout(F.relu(self.fc4(x)))\n",
        "        # output with softmax\n",
        "        x = F.log_softmax(self.fc3(x),dim=1)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsjbDjAo9_6g"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)\n",
        "\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihgj3EQlNHg2"
      },
      "source": [
        "writer = SummaryWriter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhp-5VKD-rXH"
      },
      "source": [
        "writer = SummaryWriter()\n",
        "dataiter = iter(train_set)\n",
        "images, labels = dataiter.next()\n",
        "writer.add_graph(model, images.to(device))\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDpqMKNe4qbB"
      },
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "model = model.to(device)\n",
        "summary(model, input_size= (3,256,256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Nqai2rJ8J4u"
      },
      "source": [
        "from itertools import product\n",
        "parameters = dict(\n",
        "    dropout = [0.1, 0.3, 0.5],\n",
        "    lr = [0.01, 0.001],\n",
        "    batch_size = [32,64,128],\n",
        "    shuffle = [True, False]\n",
        ")\n",
        "\n",
        "param_val = [v for v in parameters.values()]\n",
        "print(param_val)\n",
        "\n",
        "for dropout, lr,batch_size, shuffle in product(*param_val):\n",
        "    print(dropout, lr, batch_size, shuffle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGEwkomn8mti"
      },
      "source": [
        "def get_num_correct(preds, labels):\n",
        "    return preds.argmax(dim=1).eq(labels).sum().item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrjfFgxG7_os"
      },
      "source": [
        "for hyperpam_test_id, (dropout, lr,batch_size, shuffle) in enumerate(product(*param_val)):\n",
        "    print(\"Hyperparameter Test ID:\", hyperpam_test_id + 1)\n",
        "    model = Net().to(device)\n",
        "    train_set = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = shuffle)\n",
        "    optimizer = optim.Adam(model.parameters(), lr= lr)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    comment = f'dropout = {dropout} batch_size = {batch_size} lr = {lr} shuffle = {shuffle}'\n",
        "    writer = SummaryWriter(comment=comment)\n",
        "    for epoch in range(10):\n",
        "        total_loss = 0\n",
        "        total_correct = 0\n",
        "        for images, labels in train_set:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            preds = model(images)\n",
        "\n",
        "            loss = criterion(preds, labels)\n",
        "            total_loss+= loss.item()\n",
        "            total_correct+= get_num_correct(preds, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        writer.add_scalar(\"Loss\", total_loss, epoch)\n",
        "        writer.add_scalar(\"Correct\", total_correct, epoch)\n",
        "        writer.add_scalar(\"Accuracy\", total_correct/ len(train_set), epoch)\n",
        "\n",
        "        print(\"dropout:\",dropout, \"batch_size:\",batch_size, \"lr:\",lr,\"shuffle:\",shuffle)\n",
        "        print(\"epoch:\", epoch, \"total_correct:\", total_correct, \"loss:\",total_loss)\n",
        "    print(\"___________________________________________________________________\")\n",
        "\n",
        "    writer.add_hparams(\n",
        "            {\"dropout\" : dropout, \"lr\": lr, \"batch_size\": batch_size, \"shuffle\":shuffle},\n",
        "            {\n",
        "                \"accuracy\": total_correct/ len(train_set),\n",
        "                \"loss\": total_loss,\n",
        "            },\n",
        "        )\n",
        "\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8b9uqPjigfb"
      },
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir ./runs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcqMjseQ5Des"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwBLv8tXHUEh"
      },
      "source": [
        "scheduler = lr_scheduler.StepLR(optimizer, step_size = 7, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcePXfWR6y7r"
      },
      "source": [
        "history={'train_loss':[],'valid_loss':[],'train_acc':[],'valid_acc':[]}\n",
        "num_epochs=10\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss,train_correct=0.0,0\n",
        "    model.train()\n",
        "    for images, labels in train_set:\n",
        "\n",
        "        images,labels = images.to(device),labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images)\n",
        "        loss = criterion(output,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        scores, predictions = torch.max(output.data, 1)\n",
        "        train_correct += (predictions == labels).sum().item()\n",
        "\n",
        "    valid_loss, val_correct = 0.0, 0\n",
        "    model.eval()\n",
        "    for images, labels in val_set:\n",
        "\n",
        "        images,labels = images.to(device),labels.to(device)\n",
        "        output = model(images)\n",
        "        loss=criterion(output,labels)\n",
        "        valid_loss+=loss.item()*images.size(0)\n",
        "        scores, predictions = torch.max(output.data,1)\n",
        "        val_correct+=(predictions == labels).sum().item()\n",
        "\n",
        "\n",
        "    train_loss = train_loss / len(train_set.sampler)\n",
        "    train_acc = train_correct / len(train_set.sampler)*100\n",
        "    valid_loss = valid_loss / len(val_set.sampler)\n",
        "    valid_acc = val_correct / len(val_set.sampler) * 100\n",
        "\n",
        "    print(\"Epoch:{}/{} \\t average training loss:{:.4f} average validation loss:{:.4f} \\t average training acc.:{:.2f} %  average validation acc.:{:.2f} %\".format(epoch + 1, num_epochs,\n",
        "                                                                                             train_loss,\n",
        "                                                                                             valid_loss,\n",
        "                                                                                             train_acc,\n",
        "                                                                                            valid_acc))\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['valid_loss'].append(valid_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['valid_acc'].append(valid_acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLAD6-qhSSwz"
      },
      "source": [
        "import shutil\n",
        "shutil.rmtree(\"./runs/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8VIcPv1W9xG"
      },
      "source": [
        "Here you can define a confusion matrix and later reflect within tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c1MprBTiglJ"
      },
      "source": [
        " nb_classes = 2\n",
        "\n",
        " confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
        " with torch.no_grad():\n",
        "     for i, data in enumerate(val_set, 0):\n",
        "         images, labels = data\n",
        "         images = images.to(device)\n",
        "         labels = labels.to(device)\n",
        "         outputs = model(images)\n",
        "         _, preds = torch.max(outputs, 1)\n",
        "         for t, p in zip(labels.view(-1), preds.view(-1)):\n",
        "                 confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "cm = confusion_matrix.numpy()\n",
        "cm = cm / cm.sum(axis=1)\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "classes = (\"bengin\", \"malware\")\n",
        "\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "heatmap = sns.heatmap( cm, annot=True, cmap=\"gray\" ,\n",
        "                       xticklabels = classes, yticklabels=classes)\n",
        "\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veiXjapscxxy"
      },
      "source": [
        "writer = SummaryWriter()\n",
        "writer.add_figure(\"Confusion Matrix\", fig)\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAZnDbUKsja4"
      },
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir ./runs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6qzZB996aVH"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}